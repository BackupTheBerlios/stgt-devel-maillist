<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Stgt-devel] [Patch 0/4] bs_mmap / kreq_send threading
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/stgt-devel/2007-November/index.html" >
   <LINK REL="made" HREF="mailto:stgt-devel%40lists.berlios.de?Subject=Re%3A%20%5BStgt-devel%5D%20%5BPatch%200/4%5D%20bs_mmap%20/%20kreq_send%20threading&In-Reply-To=%3C20071116183359J.fujita.tomonori%40lab.ntt.co.jp%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001136.html">
   <LINK REL="Next"  HREF="001137.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Stgt-devel] [Patch 0/4] bs_mmap / kreq_send threading</H1>
    <B>FUJITA Tomonori</B> 
    <A HREF="mailto:stgt-devel%40lists.berlios.de?Subject=Re%3A%20%5BStgt-devel%5D%20%5BPatch%200/4%5D%20bs_mmap%20/%20kreq_send%20threading&In-Reply-To=%3C20071116183359J.fujita.tomonori%40lab.ntt.co.jp%3E"
       TITLE="[Stgt-devel] [Patch 0/4] bs_mmap / kreq_send threading">fujita.tomonori at lab.ntt.co.jp
       </A><BR>
    <I>Fri Nov 16 10:33:59 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001136.html">[Stgt-devel] [Patch 0/4] bs_mmap / kreq_send threading
</A></li>
        <LI>Next message: <A HREF="001137.html">[Stgt-devel] What is the status of iSER patches?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1157">[ date ]</a>
              <a href="thread.html#1157">[ thread ]</a>
              <a href="subject.html#1157">[ subject ]</a>
              <a href="author.html#1157">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Tue, 6 Nov 2007 01:27:10 +0900
FUJITA Tomonori &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/stgt-devel">tomof at acm.org</A>&gt; wrote:

&gt;<i> Sorry about the delay.
</I>&gt;<i> 
</I>&gt;<i> On Tue, 23 Oct 2007 16:14:01 -0500
</I>&gt;<i> Robert Jennings &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/stgt-devel">rcj at linux.vnet.ibm.com</A>&gt; wrote:
</I>&gt;<i> 
</I>&gt;<i> &gt; A little threading work like we have in bs_sync for bs_mmap.  The mmap
</I>&gt;<i> &gt; can be taken care of with a small pool of worker threads this way.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; After the mmap the completed commands enter back into the main tgtd
</I>&gt;<i> &gt; thread and would be sent to the kernel for in-kernel drivers through
</I>&gt;<i> &gt; kreq_send.  Unfortunately kreq_send can sleep in blk_rq_map_user or
</I>&gt;<i> &gt; scsi_map_user_pages and hold up processing in tgtd.  So kreq_send can
</I>&gt;<i> &gt; add the command to a list and hand it off to a small thread pool to
</I>&gt;<i> &gt; process sending the replies to the kernel.
</I>&gt;<i> 
</I>&gt;<i> tgtd is blocked when the data of a file isn't in page cache. So how
</I>&gt;<i> about worker threads does access to the data before tgtd calls
</I>&gt;<i> kreq_send? It works like the flush web server.
</I>
Have you measured the performance with your patch?

I tried the worker thread idea, but the performance drops (probabaly
due to pipe notification overveheads).


diff --git a/usr/Makefile b/usr/Makefile
index addf5be..7339dfc 100644
--- a/usr/Makefile
+++ b/usr/Makefile
@@ -29,6 +29,7 @@ ifneq ($(IBMVIO),)
 CFLAGS += -DIBMVIO -DUSE_KERNEL
 TGTD_OBJS += $(addprefix ibmvio/, ibmvio.o)
 TGTD_OBJS += bs_mmap.o tgtif.o
+LIBS += -lpthread
 endif
 
 ifneq ($(ISCSI),)
diff --git a/usr/bs_mmap.c b/usr/bs_mmap.c
index ad1bb4f..e4c0b86 100644
--- a/usr/bs_mmap.c
+++ b/usr/bs_mmap.c
@@ -26,59 +26,277 @@
 #include &lt;stdlib.h&gt;
 #include &lt;string.h&gt;
 #include &lt;unistd.h&gt;
+#include &lt;pthread.h&gt;
 #include &lt;sys/mman.h&gt;
+#include &lt;sys/epoll.h&gt;
 
 #include &quot;list.h&quot;
 #include &quot;util.h&quot;
 #include &quot;tgtd.h&quot;
 #include &quot;scsi.h&quot;
 
-static int bs_mmap_open(struct scsi_lu *lu, char *path, int *fd, uint64_t *size)
-{
-	*fd = backed_file_open(path, O_RDWR| O_LARGEFILE, size);
+#define NR_WORKER_THREADS	4
 
-	return *fd &gt;= 0 ? 0 : *fd;
-}
+struct bs_mmap_info {
+	pthread_t ack_thread;
+	pthread_t worker_thread[NR_WORKER_THREADS];
 
-static void bs_mmap_close(struct scsi_lu *lu)
+	/* protected by pipe */
+	struct list_head ack_list;
+
+	pthread_cond_t finished_cond;
+	pthread_mutex_t finished_lock;
+	struct list_head finished_list;
+
+	/* wokers sleep on this and signaled by tgtd */
+	pthread_cond_t pending_cond;
+	/* locked by tgtd and workers */
+	pthread_mutex_t pending_lock;
+	/* protected by pending_lock */
+	struct list_head pending_list;
+
+	int command_fd[2];
+	int done_fd[2];
+
+	int stop;
+};
+
+static void *bs_mmap_ack_fn(void *arg)
 {
-	close(lu-&gt;fd);
+	struct bs_mmap_info *info = arg;
+	int command, ret, nr;
+	struct scsi_cmd *cmd;
+
+retry:
+	ret = read(info-&gt;command_fd[0], &amp;command, sizeof(command));
+	if (ret &lt; 0) {
+		eprintf(&quot;ack pthread will be dead, %m\n&quot;);
+		if (errno == EAGAIN || errno == EINTR)
+			goto retry;
+
+		goto out;
+	}
+
+	pthread_mutex_lock(&amp;info-&gt;finished_lock);
+retest:
+	if (list_empty(&amp;info-&gt;finished_list)) {
+		pthread_cond_wait(&amp;info-&gt;finished_cond, &amp;info-&gt;finished_lock);
+		goto retest;
+	}
+
+	while (!list_empty(&amp;info-&gt;finished_list)) {
+		cmd = list_entry(info-&gt;finished_list.next,
+				 struct scsi_cmd, bs_list);
+
+		dprintf(&quot;found %p\n&quot;, cmd);
+
+		list_del(&amp;cmd-&gt;bs_list);
+		list_add(&amp;cmd-&gt;bs_list, &amp;info-&gt;ack_list);
+	}
+
+	pthread_mutex_unlock(&amp;info-&gt;finished_lock);
+
+	nr = 1;
+rewrite:
+	ret = write(info-&gt;done_fd[1], &amp;nr, sizeof(nr));
+	if (ret &lt; 0) {
+		eprintf(&quot;can't ack tgtd, %m\n&quot;);
+		if (errno == EAGAIN || errno == EINTR)
+			goto rewrite;
+
+		goto out;
+	}
+
+	goto retry;
+out:
+	return NULL;
 }
 
 #define pgcnt(size, offset)	((((size) + ((offset) &amp; (pagesize - 1))) + (pagesize - 1)) &gt;&gt; pageshift)
 
-static int bs_mmap_cmd_submit(struct scsi_cmd *cmd)
+static void *bs_mmap_worker_fn(void *arg)
 {
-	int fd = cmd-&gt;dev-&gt;fd, ret = 0;
-	void *p;
-	uint64_t addr;
+	int ret = 0;
+	struct bs_mmap_info *info = arg;
+	struct scsi_cmd *cmd;
 	uint32_t length;
 
-	if (cmd-&gt;scb[0] == SYNCHRONIZE_CACHE ||
-	    cmd-&gt;scb[0] == SYNCHRONIZE_CACHE_16)
-		return fsync(fd);
+	while (1) {
+		uint64_t addr;
+		char *p, dummy;
+		int i, nr;
 
-	length = (scsi_get_data_dir(cmd) == DATA_WRITE) ?
-		scsi_get_out_length(cmd) : scsi_get_in_length(cmd);
+		pthread_mutex_lock(&amp;info-&gt;pending_lock);
+	retest:
+		if (list_empty(&amp;info-&gt;pending_list)) {
+			pthread_cond_wait(&amp;info-&gt;pending_cond, &amp;info-&gt;pending_lock);
+			if (info-&gt;stop) {
+				pthread_mutex_unlock(&amp;info-&gt;pending_lock);
+				break;
+			}
+			goto retest;
+		}
 
-	p = mmap64(NULL, pgcnt(length, cmd-&gt;offset) &lt;&lt; pageshift,
-		   PROT_READ | PROT_WRITE, MAP_SHARED, fd,
-		   cmd-&gt;offset &amp; ~((1ULL &lt;&lt; pageshift) - 1));
-	if (p == MAP_FAILED) {
-		ret = -EINVAL;
-		eprintf(&quot;%u %&quot; PRIu64 &quot;\n&quot;, length, cmd-&gt;offset);
+		cmd = list_entry(info-&gt;pending_list.next,
+				 struct scsi_cmd, bs_list);
+
+		dprintf(&quot;got %p\n&quot;, cmd);
+
+		list_del(&amp;cmd-&gt;bs_list);
+		pthread_mutex_unlock(&amp;info-&gt;pending_lock);
+
+		if (cmd-&gt;scb[0] == SYNCHRONIZE_CACHE ||
+		    cmd-&gt;scb[0] == SYNCHRONIZE_CACHE_16) {
+			ret = fsync(cmd-&gt;dev-&gt;fd);
+			goto done;
+		}
+
+		ret = 0;
+
+		if (cmd-&gt;data_dir == DATA_WRITE) {
+			addr = (unsigned long)scsi_get_out_buffer(cmd);
+			length = scsi_get_out_length(cmd);
+		} else {
+			addr = (unsigned long)scsi_get_in_buffer(cmd);
+			length = scsi_get_in_length(cmd);
+		}
+
+		nr = pgcnt(length, (addr &amp; (pagesize - 1)));
+		addr &amp;= ~(pagesize - 1);
+
+		p = (void *)(unsigned long)addr;
+		for (i = 0; i &lt; nr; i++) {
+			dummy = *p;
+			p += pagesize;
+		}
+	done:
+		if (ret) {
+			eprintf(&quot;io error %p %x %d %d, %m\n&quot;,
+				cmd, cmd-&gt;scb[0], ret, cmd-&gt;offset);
+			scsi_set_result(cmd, SAM_STAT_CHECK_CONDITION);
+			sense_data_build(cmd, MEDIUM_ERROR, ASC_READ_ERROR);
+		} else
+			scsi_set_result(cmd, SAM_STAT_GOOD);
+
+		pthread_mutex_lock(&amp;info-&gt;finished_lock);
+		list_add(&amp;cmd-&gt;bs_list, &amp;info-&gt;finished_list);
+		pthread_mutex_unlock(&amp;info-&gt;finished_lock);
+
+		pthread_cond_signal(&amp;info-&gt;finished_cond);
 	}
 
-	addr = (unsigned long)p + (cmd-&gt;offset &amp; (pagesize - 1));
+	return NULL;
+}
 
-	if (scsi_get_data_dir(cmd) == DATA_WRITE)
-		scsi_set_out_buffer(cmd, (void *)(unsigned long)addr);
-	else if (scsi_get_data_dir(cmd) == DATA_READ)
-		scsi_set_in_buffer(cmd, (void *)(unsigned long)addr);
+static void bs_mmap_handler(int fd, int events, void *data)
+{
+	struct bs_mmap_info *info = data;
+	struct scsi_cmd *cmd;
+	int nr_events, ret;
 
-	dprintf(&quot;%&quot; PRIx64 &quot; %u %&quot; PRIu64 &quot;\n&quot;, addr, length, cmd-&gt;offset);
+	ret = read(info-&gt;done_fd[0], &amp;nr_events, sizeof(nr_events));
+	if (ret &lt; 0) {
+		eprintf(&quot;wrong wakeup\n&quot;);
+		return;
+	}
+
+	while (!list_empty(&amp;info-&gt;ack_list)) {
+		cmd = list_entry(info-&gt;ack_list.next,
+				 struct scsi_cmd, bs_list);
 
-	return ret;
+		dprintf(&quot;back to tgtd, %p\n&quot;, cmd);
+
+		list_del(&amp;cmd-&gt;bs_list);
+		target_cmd_io_done(cmd, scsi_get_result(cmd));
+	}
+
+	write(info-&gt;command_fd[1], &amp;nr_events, sizeof(nr_events));
+}
+
+static int bs_mmap_open(struct scsi_lu *lu, char *path, int *fd, uint64_t *size)
+{
+	int i, ret;
+	struct bs_mmap_info *info =
+		(struct bs_mmap_info *)((char *)lu + sizeof(*lu));
+
+	INIT_LIST_HEAD(&amp;info-&gt;ack_list);
+	INIT_LIST_HEAD(&amp;info-&gt;finished_list);
+	INIT_LIST_HEAD(&amp;info-&gt;pending_list);
+
+	*fd = backed_file_open(path, O_RDWR| O_LARGEFILE, size);
+	if (*fd &lt; 0)
+		return *fd;
+
+	pthread_cond_init(&amp;info-&gt;finished_cond, NULL);
+	pthread_cond_init(&amp;info-&gt;pending_cond, NULL);
+
+	pthread_mutex_init(&amp;info-&gt;finished_lock, NULL);
+	pthread_mutex_init(&amp;info-&gt;pending_lock, NULL);
+
+	ret = pipe(info-&gt;command_fd);
+	if (ret)
+		goto close_dev_fd;
+
+	ret = pipe(info-&gt;done_fd);
+	if (ret)
+		goto close_command_fd;
+
+	ret = tgt_event_add(info-&gt;done_fd[0], EPOLLIN, bs_mmap_handler, info);
+	if (ret)
+		goto close_done_fd;
+
+	ret = pthread_create(&amp;info-&gt;ack_thread, NULL, bs_mmap_ack_fn, info);
+	if (ret)
+		goto event_del;
+
+	for (i = 0; i &lt; ARRAY_SIZE(info-&gt;worker_thread); i++) {
+		ret = pthread_create(&amp;info-&gt;worker_thread[i], NULL,
+				     bs_mmap_worker_fn, info);
+	}
+
+	write(info-&gt;command_fd[1], &amp;ret, sizeof(ret));
+
+	return 0;
+event_del:
+	tgt_event_del(info-&gt;done_fd[0]);
+close_done_fd:
+	close(info-&gt;done_fd[0]);
+	close(info-&gt;done_fd[1]);
+close_command_fd:
+	close(info-&gt;command_fd[0]);
+	close(info-&gt;command_fd[1]);
+close_dev_fd:
+	close(*fd);
+
+	pthread_cond_destroy(&amp;info-&gt;finished_cond);
+	pthread_cond_destroy(&amp;info-&gt;pending_cond);
+	pthread_mutex_destroy(&amp;info-&gt;finished_lock);
+	pthread_mutex_destroy(&amp;info-&gt;pending_lock);
+
+	return -1;
+}
+
+static void bs_mmap_close(struct scsi_lu *lu)
+{
+	int i;
+	struct bs_mmap_info *info =
+		(struct bs_mmap_info *)((char *)lu + sizeof(*lu));
+
+	pthread_cancel(info-&gt;ack_thread);
+	pthread_join(info-&gt;ack_thread, NULL);
+
+	info-&gt;stop = 1;
+	pthread_cond_broadcast(&amp;info-&gt;pending_cond);
+
+	for (i = 0; i &lt; ARRAY_SIZE(info-&gt;worker_thread); i++)
+		pthread_join(info-&gt;worker_thread[i], NULL);
+
+	pthread_cond_destroy(&amp;info-&gt;finished_cond);
+	pthread_cond_destroy(&amp;info-&gt;pending_cond);
+	pthread_mutex_destroy(&amp;info-&gt;finished_lock);
+	pthread_mutex_destroy(&amp;info-&gt;pending_lock);
+
+	close(lu-&gt;fd);
 }
 
 static int bs_mmap_cmd_done(struct scsi_cmd *cmd)
@@ -109,7 +327,59 @@ static int bs_mmap_cmd_done(struct scsi_cmd *cmd)
 	return err;
 }
 
+static int bs_mmap_cmd_submit(struct scsi_cmd *cmd)
+{
+	struct scsi_lu *lu = cmd-&gt;dev;
+	struct bs_mmap_info *info =
+		(struct bs_mmap_info *)((char *)lu + sizeof(*lu));
+	int fd;
+	uint32_t length;
+	uint64_t addr;
+	void *p;
+
+	fd = cmd-&gt;dev-&gt;fd;
+	length = 0;
+
+	if (cmd-&gt;scb[0] == SYNCHRONIZE_CACHE ||
+	    cmd-&gt;scb[0] == SYNCHRONIZE_CACHE_16)
+		goto queuing;
+
+	if (cmd-&gt;data_dir == DATA_WRITE)
+		length = scsi_get_out_length(cmd);
+	else
+		length = scsi_get_in_length(cmd);
+
+	p = mmap64(NULL, pgcnt(length, cmd-&gt;offset) &lt;&lt; pageshift,
+		   PROT_READ | PROT_WRITE, MAP_SHARED, fd,
+		   cmd-&gt;offset &amp; ~((1ULL &lt;&lt; pageshift) - 1));
+	if (p == MAP_FAILED)
+		return EIO;
+
+	addr = (unsigned long)p + (cmd-&gt;offset &amp; (pagesize - 1));
+
+	if (scsi_get_data_dir(cmd) == DATA_WRITE)
+		scsi_set_out_buffer(cmd, (void *)(unsigned long)addr);
+	else
+		scsi_set_in_buffer(cmd, (void *)(unsigned long)addr);
+
+	scsi_set_result(cmd, SAM_STAT_GOOD);
+
+queuing:
+	pthread_mutex_lock(&amp;info-&gt;pending_lock);
+
+	list_add(&amp;cmd-&gt;bs_list, &amp;info-&gt;pending_list);
+
+	pthread_mutex_unlock(&amp;info-&gt;pending_lock);
+
+	pthread_cond_signal(&amp;info-&gt;pending_cond);
+
+	set_cmd_async(cmd);
+
+	return 0;
+}
+
 struct backingstore_template mmap_bst = {
+	.bs_datasize		= sizeof(struct bs_mmap_info),
 	.bs_open		= bs_mmap_open,
 	.bs_close		= bs_mmap_close,
 	.bs_cmd_submit		= bs_mmap_cmd_submit,

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001136.html">[Stgt-devel] [Patch 0/4] bs_mmap / kreq_send threading
</A></li>
	<LI>Next message: <A HREF="001137.html">[Stgt-devel] What is the status of iSER patches?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1157">[ date ]</a>
              <a href="thread.html#1157">[ thread ]</a>
              <a href="subject.html#1157">[ subject ]</a>
              <a href="author.html#1157">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/stgt-devel">More information about the Stgt-devel
mailing list</a><br>
</body></html>
